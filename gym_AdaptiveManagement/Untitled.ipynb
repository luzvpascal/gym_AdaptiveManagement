{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a27bb46-f84e-40c2-9922-53f268ebcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from AdaptiveManagement import AdaptiveManagement\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "transition_models = np.array([\n",
    "                        1, 0, 0, 1,\n",
    "                        0.9, 0.1, 0, 1,\n",
    "                        1, 0, 0, 1,\n",
    "                        1, 0, 0, 1\n",
    "                    ]).reshape(2, 2, 2, 2)\n",
    "\n",
    "reward_function = np.array([\n",
    "                  0.736, 0.735,\n",
    "                  0.736, 0.8540772\n",
    "                    ]).reshape(2, 2)\n",
    "\n",
    "env = AdaptiveManagement(transition_models,reward_function)\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3ffaa5-f30b-4c06-801a-5dd981e592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "register(\n",
    "    id='Ecosystem-v0',  # Environment ID, used to make the environment\n",
    "    entry_point='AdaptiveManagement:AdaptiveManagement',  # The entry point to your environment class\n",
    "    max_episode_steps=100,  # Max number of steps per episode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9edc485-4221-4c1e-9a3b-93d0edbf5906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27ed26b7910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the environment\n",
    "env = gym.make('Ecosystem-v0', \n",
    "               transition_models=transition_models,\n",
    "               reward_function=reward_function,\n",
    "               Tmax=100)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91774737-0d84-4a90-9768-c1f4595c8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.47368422, 0.5263158 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 2\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.44751382, 0.5524862 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 3\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.421631, 0.578369], dtype=float32)} reward= 0.735 done= False\n",
      "Step 4\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.39617172, 0.60382825], dtype=float32)} reward= 0.735 done= False\n",
      "Step 5\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.37126294, 0.62873703], dtype=float32)} reward= 0.735 done= False\n",
      "Step 6\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.34702024, 0.6529798 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 7\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.3235459, 0.6764541], dtype=float32)} reward= 0.735 done= False\n",
      "Step 8\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.3009277, 0.6990723], dtype=float32)} reward= 0.735 done= False\n",
      "Step 9\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.279238, 0.720762], dtype=float32)} reward= 0.735 done= False\n",
      "Step 10\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.25853342, 0.7414666 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 11\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.23885527, 0.7611447 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 12\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.22023006, 0.77976996], dtype=float32)} reward= 0.735 done= False\n",
      "Step 13\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.20267047, 0.79732955], dtype=float32)} reward= 0.735 done= False\n",
      "Step 14\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.18617667, 0.81382334], dtype=float32)} reward= 0.735 done= False\n",
      "Step 15\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.17073774, 0.82926226], dtype=float32)} reward= 0.735 done= False\n",
      "Step 16\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.15633316, 0.84366685], dtype=float32)} reward= 0.735 done= False\n",
      "Step 17\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.14293438, 0.8570656 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 18\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.13050634, 0.86949366], dtype=float32)} reward= 0.735 done= False\n",
      "Step 19\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.11900884, 0.88099116], dtype=float32)} reward= 0.735 done= False\n",
      "Step 20\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.10839799, 0.89160204], dtype=float32)} reward= 0.735 done= False\n",
      "Step 21\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.09862729, 0.90137273], dtype=float32)} reward= 0.735 done= False\n",
      "Step 22\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.08964875, 0.9103513 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 23\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.08141373, 0.91858625], dtype=float32)} reward= 0.735 done= False\n",
      "Step 24\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.0738738, 0.9261262], dtype=float32)} reward= 0.735 done= False\n",
      "Step 25\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.06698123, 0.93301874], dtype=float32)} reward= 0.735 done= False\n",
      "Step 26\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.06068961, 0.9393104 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 27\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.05495417, 0.9450458 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 28\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.04973205, 0.950268  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 29\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.04498255, 0.95501745], dtype=float32)} reward= 0.735 done= False\n",
      "Step 30\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.04066723, 0.95933276], dtype=float32)} reward= 0.735 done= False\n",
      "Step 31\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.03674996, 0.96325004], dtype=float32)} reward= 0.735 done= False\n",
      "Step 32\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.03319696, 0.966803  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 33\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.02997678, 0.9700232 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 34\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.02706022, 0.9729398 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 35\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.02442028, 0.97557974], dtype=float32)} reward= 0.735 done= False\n",
      "Step 36\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.02203205, 0.9779679 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 37\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01987263, 0.9801274 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 38\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01792098, 0.982079  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 39\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01615784, 0.98384213], dtype=float32)} reward= 0.735 done= False\n",
      "Step 40\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01456559, 0.9854344 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 41\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01312815, 0.98687184], dtype=float32)} reward= 0.735 done= False\n",
      "Step 42\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.01183087, 0.98816913], dtype=float32)} reward= 0.735 done= False\n",
      "Step 43\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.0106604, 0.9893396], dtype=float32)} reward= 0.735 done= False\n",
      "Step 44\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00960459, 0.9903954 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 45\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00865244, 0.99134755], dtype=float32)} reward= 0.735 done= False\n",
      "Step 46\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00779394, 0.99220604], dtype=float32)} reward= 0.735 done= False\n",
      "Step 47\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00702002, 0.99298   ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 48\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00632246, 0.99367756], dtype=float32)} reward= 0.735 done= False\n",
      "Step 49\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00569381, 0.9943062 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 50\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00512735, 0.9948726 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 51\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00461698, 0.995383  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 52\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.0041572, 0.9958428], dtype=float32)} reward= 0.735 done= False\n",
      "Step 53\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00374304, 0.99625695], dtype=float32)} reward= 0.735 done= False\n",
      "Step 54\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00337, 0.99663], dtype=float32)} reward= 0.735 done= False\n",
      "Step 55\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00303402, 0.996966  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 56\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00273145, 0.99726856], dtype=float32)} reward= 0.735 done= False\n",
      "Step 57\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00245897, 0.997541  ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 58\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00221362, 0.9977864 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 59\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.0019927, 0.9980073], dtype=float32)} reward= 0.735 done= False\n",
      "Step 60\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00179379, 0.9982062 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 61\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.0016147, 0.9983853], dtype=float32)} reward= 0.735 done= False\n",
      "Step 62\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00145346, 0.99854654], dtype=float32)} reward= 0.735 done= False\n",
      "Step 63\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00130831, 0.9986917 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 64\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00117763, 0.9988224 ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 65\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([0.00105999, 0.99894   ], dtype=float32)} reward= 0.735 done= False\n",
      "Step 66\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([9.5409376e-04, 9.9904591e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 67\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([8.587663e-04, 9.991412e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 68\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([7.7295606e-04, 9.9922705e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 69\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([6.9571426e-04, 9.9930429e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 70\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([6.261864e-04, 9.993738e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 71\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([5.6360307e-04, 9.9943638e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 72\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([5.072713e-04, 9.994927e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 73\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([4.5656736e-04, 9.9954343e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 74\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([4.1092938e-04, 9.9958909e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 75\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([3.6985165e-04, 9.9963015e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 76\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([3.328788e-04, 9.996671e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 77\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.9960088e-04, 9.9970043e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 78\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.6964888e-04, 9.9973035e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 79\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.4269053e-04, 9.9975729e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 80\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.1842678e-04, 9.9978155e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 81\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.965884e-04, 9.998034e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 82\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.7693304e-04, 9.9982309e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 83\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.5924255e-04, 9.9984074e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 84\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.4332058e-04, 9.9985665e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 85\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.2899037e-04, 9.9987102e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 86\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.1609283e-04, 9.9988389e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 87\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([1.0448476e-04, 9.9989551e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 88\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([9.4037263e-05, 9.9990594e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 89\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([8.4634332e-05, 9.9991536e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 90\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([7.617154e-05, 9.999238e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 91\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([6.8554917e-05, 9.9993145e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 92\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([6.1699844e-05, 9.9993831e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 93\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([5.5530203e-05, 9.9994445e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 94\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([4.9977461e-05, 9.9995005e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 95\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([4.4979937e-05, 9.9995500e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 96\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([4.0482126e-05, 9.9995953e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 97\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([3.6434063e-05, 9.9996358e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 98\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([3.2790776e-05, 9.9996722e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 99\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.9511795e-05, 9.9997050e-01], dtype=float32)} reward= 0.735 done= False\n",
      "Step 100\n",
      "Action:  1\n",
      "obs= {'state': 0, 'belief': array([2.6560694e-05, 9.9997342e-01], dtype=float32)} reward= 0.735 done= True\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "env = AdaptiveManagement(transition_models,reward_function)\n",
    "obs, _ = env.reset()\n",
    "env.true_model_index = 1\n",
    "env.true_transition_model = env.transition_models[env.true_model_index]\n",
    "n_steps = 100\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", terminated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
